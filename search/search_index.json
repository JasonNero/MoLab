{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MoLab's documentation!","text":""},{"location":"#overview","title":"Overview","text":"<p>Motion Lab (MoLab) is an innovative framework that supports producers and animators in the pre-production of character animations through machine learning. Traditional animation and motion capture are often time-consuming or have specific limitations, which is why MoLab envisions translating text descriptions into animated sequences and editing existing sequences. By utilizing the CondMDI model, efficient and consistent generation of movements is enabled. The layer-based concept of the MoLab Sequencer allows for intuitive and non-destructive work with both hand-animated and generated motion sequences.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Text To Motion: Describe the motion you want to see</li> <li>In-Betweening: Input your keyposes and let AI fill the gaps</li> <li>Motion Composition: Compose sequences and generate transitions</li> </ul>"},{"location":"#limitations","title":"Limitations","text":"<ul> <li>Fixed skeleton with 22 joints</li> <li>Maximum sequence length of 196 frames and a fixed framerate of 20 Hz</li> <li>Text conditioning is limited to the entire sequence and the entire body</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>See the Installation Guide to set up the backend and worker components.</p>"},{"location":"concept/","title":"MoLab Concept and Architecture","text":""},{"location":"concept/#folder-structure","title":"Folder Structure","text":"<p>Each component of MoLab is located in a separate directory outlined below. The <code>Justfile</code> in the root directory provides a way to automate build and test tasks across components. The terms backend and gateway are used interchangeably in the context of MoLab.</p> <pre><code>MoLab/\n\u251c\u2500\u2500 backend/            # FastAPI endpoint for inference\n\u251c\u2500\u2500 models/condmdi/     # CondMDI fork with new features and improvements\n\u2502   \u2514\u2500\u2500 README.md       # CondMDI model description and usage instructions\n\u251c\u2500\u2500 frontend/           # Godot User Interface for MoLab Sequencer\n\u251c\u2500\u2500 dcc/                # DCC plugins (Maya, Blender, etc.)\n\u251c\u2500\u2500 docs/               # Documentation for the project\n\u251c\u2500\u2500 Justfile            # Automate build/test tasks across components\n\u2514\u2500\u2500 README.md           # Project description and setup instructions\n</code></pre>"},{"location":"concept/#overview","title":"Overview","text":"<p>Conceptually, MoLab consists of these four components: the MoLab Sequencer, Gateway, Worker and DCC Plugin.</p> <p>The Worker contains the machine learning model and is responsible for generating motion sequences based on text descriptions and the Gateway is the interface between the worker and the client. It is responsible for distributing the inference requests to a free worker instance and returning the results.</p> <p>Currently, there are two available clients: the DCC Plugin and the MoLab Sequencer. Both allow the user to send inference requests and receive the generated motion sequences but have different use-cases. The former is a plugin for the 3D software Autodesk Maya and allows a direct integration into an animators workflow. The latter is a standalone application aimed at higher-level use-cases like pre-production and provides a timeline-based user interface for editing and composing motion sequences.</p> <p></p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>If you want to deploy MoLab in a production environment, you can follow the instructions below to set up the backend and worker components on a server. How you distribute the clients is up to you.</p>"},{"location":"deployment/#option-1-bare-metal","title":"Option 1: Bare Metal","text":"<p>Let's assume we want to run the worker and backend on the same server for now, which is the simplest setup, because we can just follow the regular installation guide. In a second step we'll add more workers on different servers.</p> <p>On the first server, clone the repository and make sure you have the necessary dependencies installed (uv and just). Then setup the Python environment and download the model weights:</p> <pre><code>just install\njust download\n</code></pre> <p>You can then run the backend and worker components on the server:</p> <pre><code>just run-backend\njust run-worker\n</code></pre> <p>By default, the backend server will listen on <code>0.0.0.0:8000</code>, which means it will accept connections from any IP address.</p>"},{"location":"deployment/#adding-more-workers","title":"Adding More Workers","text":"<p>Now, let's add another worker on a different server. Make sure you have the repository cloned and setup as before.</p> <p>By default, a worker will connect to the backend server running on <code>localhost:8000</code>. If you want to change this behavior, you can set the <code>MOLAB_GATEWAY_HOST</code> and <code>MOLAB_GATEWAY_PORT</code> environment variables accordingly:</p> <pre><code>export MOLAB_GATEWAY_HOST=your-host\nexport MOLAB_GATEWAY_PORT=your-port\n</code></pre> <p>Then you can start the worker:</p> <pre><code>just run-worker\n</code></pre> <p>You should see a message that the worker has connected to the backend server, which means it is ready to accept inference requests.</p> <p>For easier deployment, you can also use Docker, which we'll cover in the next section.</p>"},{"location":"deployment/#option-2-docker","title":"Option 2: Docker","text":"<p>Each component has a <code>Dockerfile</code> in its root directory, which follows a similar setup as the bare metal setup, but with the added benefit of containerization.</p>"},{"location":"deployment/#gateway-image","title":"Gateway Image","text":"<p>To build the backend image, you can run the following command:</p> <pre><code>cd backend\ndocker build -t molab_backend:latest .\n</code></pre> <p>Then export the image to a tar file and transfer it to your server:</p> <pre><code>docker save -o molab_backend.tar molab_backend:latest\n</code></pre> <p>On the server, load the image and start the container:</p> <pre><code>docker load -i molab_backend.tar\ndocker run -p 8000:8000 molab_backend\n</code></pre> <p>This will start the backend server and expose it on port 8000.</p>"},{"location":"deployment/#worker-image","title":"Worker Image","text":"<p>If you haven't done so already, make download the model weights:</p> <pre><code>just download\n</code></pre> <p>Now you can follow the same steps as above, but for the worker component:</p> <pre><code>cd models/condmdi\ndocker build -t molab_worker:latest .\n</code></pre> <p>Then export the image to a tar file and transfer it to your server:</p> <pre><code>docker save -o molab_worker.tar molab_worker:latest\n</code></pre> <p>On the server, load the image and start the container, this time with the necessary environment variables pointing to the backend server:</p> <pre><code>docker load -i molab_worker.tar\ndocker run molab_worker \\\n-e MOLAB_GATEWAY_HOST=your-host \\\n-e MOLAB_GATEWAY_PORT=your-port\n</code></pre>"},{"location":"deployment/#docker-compose","title":"<code>docker-compose</code>","text":"<p>As an alternative approach to the steps outline above, you can also use\u00a0<code>docker-compose</code>.</p> <p>There is a <code>docker-compose.yml</code> file in the root directory as example that you can use to start the backend and two worker instances locally. It specifies the necessary environment variables for the backend and workers, as well as the GPU requirement for the workers.</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"api-reference/api-example/","title":"API Usage Example","text":"<p>As example on how to use the API for inference, we'll first take a look at the <code>dcc.molab_maya.client</code> module to see how to connect to the backend and send a job for inference.</p> <p>Then we will look at some methods in <code>dcc.molab_maya.motion_io</code> to see how to handle input and output motions.</p> <p>Tip</p> <p>A Blender integration is an open issue and a good opportunity for a contribution!</p>"},{"location":"api-reference/api-example/#collect-input-motion","title":"Collect Input Motion","text":"<p>To be able to pass motion data around, we need to understand the motion format at input and output. The input format <code>packed_motion</code> is a bit special due to its sparse nature, while the output format is more straightforward.</p> <p>Part of the <code>InferenceArgs</code> is the <code>packed_motion</code> field. This is a dictionary mapping frame indices to packed poses, where a packed pose contains the root position followed by all 22 joint rotations, so it has a shape of <code>(23, 3)</code>. Values stored as <code>NaN</code> indicate sparse keyframes aka missing joint information. The <code>NaN</code> values are later converted to a joint mask for the inference model.</p> <p>For details on how to pack keyframes, see the <code>pack_keyframes</code> function of the Maya integration below. It takes a list of positions and a list of rotations that can contain <code>NaN</code> values and converts them to a <code>packed_motion</code> dictionary.</p>"},{"location":"api-reference/api-example/#dcc.molab_maya.motion_io.pack_keyframes","title":"<code>dcc.molab_maya.motion_io.pack_keyframes(positions, rotations, verbose=False)</code>","text":"<p>Pack the keyframes into a more compact format.</p> <p>Parameters:</p> Name Type Description Default <code>positions</code> <code>ndarray</code> <p>(N, 22, 3) array of joint positions</p> required <code>rotations</code> <code>ndarray</code> <p>(N, 22, 3) array of joint rotations</p> required <code>verbose</code> <p>Whether to print the compacted frames.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>packed_motion</code> <code>dict[int, list]</code> <p>A dictionary mapping frame indices to packed poses.</p> Source code in <code>dcc/molab_maya/motion_io.py</code> <pre><code>def pack_keyframes(\n    positions: np.ndarray, rotations: np.ndarray, verbose=False\n) -&gt; dict[int, list]:\n    \"\"\"Pack the keyframes into a more compact format.\n\n    Arguments:\n        positions: (N, 22, 3) array of joint positions\n        rotations: (N, 22, 3) array of joint rotations\n        verbose: Whether to print the compacted frames.\n\n    Returns:\n        packed_motion: A dictionary mapping frame indices to packed poses.\n    \"\"\"\n    root_pos = positions[:, 0]\n    frame_mask_pos = ~np.isnan(root_pos).any(axis=(1))\n    frame_mask_rot = ~np.isnan(rotations).any(axis=(1, 2))\n\n    # Combine all frames that have non-nan values\n    frame_mask = frame_mask_pos | frame_mask_rot\n    valid_frames = np.where(frame_mask)[0]\n\n    if verbose:\n        print(f\"Compacting to (partial) Keyposes on Frames:\\n{valid_frames}\")\n\n    packed_motion: dict = {}\n    for frame in valid_frames.tolist():\n        packed_motion[frame] = np.ones((23, 3)) * np.nan\n        packed_motion[frame][0] = root_pos[frame]\n        packed_motion[frame][1:] = rotations[frame]\n\n    # return packed_motion\n    return {frame: packed_motion[frame].tolist() for frame in packed_motion}\n</code></pre>"},{"location":"api-reference/api-example/#send-job-to-backend","title":"Send Job to Backend","text":"<p>Now that we have the <code>packed_motion</code> dictionary, we can craft our inference arguments and send them to the backend. Below is the documentation for <code>MoLabQClient.infer</code>, which does the following:</p> <ul> <li>Take a dictionary inference arguments (see <code>InferenceArgs</code>)</li> <li>Connect to the backend via WebSocket</li> <li>Send the job and wait for the result</li> <li>Return the result dictionary (see <code>InferenceResults</code>)</li> </ul> <p>Example</p> <p>Below is a full payload example for the <code>MoLabQClient.infer</code> method containing two keyposes on frame 0 and 42 as well as a text prompt.</p> <pre><code>{\n    \"packed_motion\": {\n        0: [\n            [0.0, 0.966035, 0.0],\n            [-2.461784, 1.602837, 3.02837205],\n            [-1.15376, -0.314741, -3.40727],\n            [1.123194, -0.57072, 9.6684651],\n            [5.430555, 12.008284, 3.450807],\n            [nan, nan, nan],\n            [0.699208, 0.478575, -4.624878],\n            [-0.736858, 0.472722, 8.868751],\n            [1.046311, 0.36474, 4.513838],\n            [nan, nan, nan],\n            [-2.097379, 0.002352, 1.437366],\n            [6.395149, -0.91336201, -23.965065],\n            [0.203726, -2.443971, 29.728936],\n            [0.572847, 0.573686, -19.469958],\n            [nan, nan, nan],\n            [-13.751465, 5.598898, -3.18948],\n            [-67.052628, -7.37833, -6.440387],\n            [-23.210149, -25.2472202, 7.097196],\n            [nan, nan, nan],\n            [7.634399, -1.97200502, -1.282972],\n            [69.428653, 6.069861, -6.181875],\n            [10.862561, 27.296937, 3.88993195],\n            [nan, nan, nan],\n        ],\n        42: [\n            [-0.339078, 0.965653, 2.350223],\n            [-2.312009, 2.433177, -1.82445502],\n            [3.73811, -1.674805, -25.165169],\n            [-8.567456, 1.59987202, 19.844779],\n            [17.532981, 30.0588684, 19.2560214],\n            [nan, nan, nan],\n            [-0.172514997, -1.10521, 10.58829],\n            [1.696808, -0.023274, 7.59669],\n            [-2.601748, -8.717805, 7.61317205],\n            [nan, nan, nan],\n            [-1.07286802, -0.173289, 5.375921],\n            [4.720364, -0.666077, -21.613376],\n            [5.729873, 8.757588, 32.877729],\n            [-4.825603, 6.19364, -15.489852],\n            [nan, nan, nan],\n            [-12.313785, 3.51262305, -3.91042395],\n            [-60.028995, -1.354013, -10.010531],\n            [-21.320532, -21.962499, 4.864524],\n            [nan, nan, nan],\n            [1.25623, 5.556887, 2.128571],\n            [49.840547, 17.558537, 0.472948984],\n            [1.83383397, 29.8473266, 2.299576],\n            [nan, nan, nan],\n        ],\n    },\n    \"text_prompt\": \"A person walks forward\",\n    \"num_samples\": 3,\n    \"type\": \"infer\",\n}\n</code></pre>"},{"location":"api-reference/api-example/#dcc.molab_maya.qclient.MoLabQClient.infer","title":"<code>dcc.molab_maya.qclient.MoLabQClient.infer(inference_args)</code>","text":"<p>Sends an inference request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>inference_args</code> <code>dict</code> <p>The data to be sent for inference.</p> required Source code in <code>dcc/molab_maya/qclient.py</code> <pre><code>def infer(self, inference_args):\n    \"\"\"\n    Sends an inference request to the backend.\n\n    Args:\n        inference_args (dict): The data to be sent for inference.\n    \"\"\"\n    print(\"Sending inference request ...\")\n    inference_args[\"type\"] = \"infer\"\n    self.websocket.sendTextMessage(json.dumps(inference_args))\n</code></pre>"},{"location":"api-reference/api-example/#apply-output-motion","title":"Apply Output Motion","text":"<p>The <code>InferenceResults</code> contains <code>root_positions</code> and <code>joint_rotations</code> fields of shape <code>(S, F, 3)</code> and <code>(S, F, 22, 3)</code> respectively, where <code>S</code> is the sample count, <code>F</code> is the number of frames.</p> <p>For debugging purposes the results also contain the input motion as <code>obs_root_positions</code> and <code>obs_joint_rotations</code>.</p> <p>In the <code>_apply_motion</code> function below, we show how to apply the output motion to a character rig in Maya.</p>"},{"location":"api-reference/api-example/#dcc.molab_maya.motion_io._apply_motion","title":"<code>dcc.molab_maya.motion_io._apply_motion(skeleton_group, root_pos, rotations, joint_mask=None, start_frame=1, name='sample')</code>","text":"<p>Apply the motion to the skeleton.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton_group</code> <code>Transform</code> <p>The skeleton group to duplicate and apply the motion to.</p> required <code>root_pos</code> <code>ndarray</code> <p>The root positions for each frame.</p> required <code>rotations</code> <code>ndarray</code> <p>The joint rotations for each frame.</p> required <code>joint_mask</code> <code>Optional[ndarray]</code> <p>The mask for each joint and frame.</p> <code>None</code> <code>start_frame</code> <code>int</code> <p>The starting frame for the motion.</p> <code>1</code> <code>name</code> <code>str</code> <p>A prefix for the duplicated skeleton.</p> <code>'sample'</code> Source code in <code>dcc/molab_maya/motion_io.py</code> <pre><code>def _apply_motion(\n    skeleton_group: pmc.nodetypes.Transform,\n    root_pos: np.ndarray,\n    rotations: np.ndarray,\n    joint_mask: Optional[np.ndarray] = None,\n    start_frame: int = 1,\n    name: str = \"sample\",\n):\n    \"\"\"Apply the motion to the skeleton.\n\n    Args:\n        skeleton_group: The skeleton group to duplicate and apply the motion to.\n        root_pos: The root positions for each frame.\n        rotations: The joint rotations for each frame.\n        joint_mask: The mask for each joint and frame.\n        start_frame: The starting frame for the motion.\n        name: A prefix for the duplicated skeleton.\n    \"\"\"\n    # Duplicate the source skeleton\n    root_grp = pmc.duplicate(skeleton_group, name=f\"{name}_{skeleton_group}\")\n    root_grp = pmc.ls(root_grp)[0]\n    root_obj = root_grp.listRelatives(children=True, type=\"joint\")[0]\n    joints = _get_hierarchy(root_obj)\n\n    # Get the start and end frames\n    start = start_frame\n    end = start_frame + root_pos.shape[0] - 1\n\n    print(f\"Applying keyframes to '{joints[0].getParent()}' [{start}-{end}]\")\n\n    for frame_time in range(start, end + 1):\n        frame_idx = frame_time - start\n        for joint_idx, joint_name in enumerate(joints):\n            # Skip joints that are not part of the mask\n            if joint_mask is not None and not joint_mask[frame_idx, joint_idx]:\n                continue\n\n            # Apply root position to the root joint\n            if joint_idx == 0:\n                pmc.setKeyframe(\n                    joint_name,\n                    time=frame_time,\n                    attribute=\"tx\",\n                    value=root_pos[frame_idx, 0],\n                )\n                pmc.setKeyframe(\n                    joint_name,\n                    time=frame_time,\n                    attribute=\"ty\",\n                    value=root_pos[frame_idx, 1],\n                )\n                pmc.setKeyframe(\n                    joint_name,\n                    time=frame_time,\n                    attribute=\"tz\",\n                    value=root_pos[frame_idx, 2],\n                )\n\n            # Apply joint rotations to the all joints\n            pmc.setKeyframe(\n                joint_name,\n                time=frame_time,\n                attribute=\"rx\",\n                value=rotations[frame_idx, joint_idx, 0],\n            )\n            pmc.setKeyframe(\n                joint_name,\n                time=frame_time,\n                attribute=\"ry\",\n                value=rotations[frame_idx, joint_idx, 1],\n            )\n            pmc.setKeyframe(\n                joint_name,\n                time=frame_time,\n                attribute=\"rz\",\n                value=rotations[frame_idx, joint_idx, 2],\n            )\n</code></pre>"},{"location":"api-reference/backend/","title":"MoLab Backend","text":"<p>Similar to a Gateway, the backend connects to clients and workers and forwards messages (jobs and results) between them. It is responsible for managing the WebSocket connections and the communication between the clients and the workers.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main","title":"<code>backend.molab_backend.main</code>","text":""},{"location":"api-reference/backend/#backend.molab_backend.main.ClientManager","title":"<code>ClientManager</code>","text":"<p>Client manager class to manage client connections.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.ClientManager.register","title":"<code>register(websocket)</code>  <code>async</code>","text":"<p>Register a new client.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The WebSocket connection for the client.</p> required <p>Returns:</p> Name Type Description <code>Connection</code> <code>Connection</code> <p>The registered client instance.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.ClientManager.unregister","title":"<code>unregister(client)</code>  <code>async</code>","text":"<p>Unregister an existing client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Connection</code> <p>The client instance to unregister.</p> required"},{"location":"api-reference/backend/#backend.molab_backend.main.Connection","title":"<code>Connection</code>  <code>dataclass</code>","text":"<p>Simple connection class to handle WebSocket connections.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The WebSocket connection for the worker or client.</p> required <code>id</code> <code>str</code> <p>The unique identifier for the connection.</p> <code>lambda: str(uuid4())()</code>"},{"location":"api-reference/backend/#backend.molab_backend.main.WorkerManager","title":"<code>WorkerManager</code>","text":"<p>Worker manager class to manage worker connections using round-robin scheduling.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.WorkerManager.get_next_worker","title":"<code>get_next_worker()</code>  <code>async</code>","text":"<p>Get the next available worker using round-robin scheduling.</p> <p>Returns:</p> Name Type Description <code>Connection</code> <code>Connection</code> <p>The next available worker instance.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.WorkerManager.register","title":"<code>register(websocket)</code>  <code>async</code>","text":"<p>Register a new worker.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The WebSocket connection for the worker.</p> required <p>Returns:</p> Name Type Description <code>Connection</code> <code>Connection</code> <p>The registered worker instance.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.WorkerManager.unregister","title":"<code>unregister(worker)</code>  <code>async</code>","text":"<p>Unregister an existing worker.</p> <p>Parameters:</p> Name Type Description Default <code>worker</code> <code>Connection</code> <p>The worker instance to unregister.</p> required"},{"location":"api-reference/backend/#backend.molab_backend.main.handle_client_request","title":"<code>handle_client_request(client, message)</code>  <code>async</code>","text":"<p>Handle client requests.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Connection</code> <p>The client instance sending the request.</p> required <code>message</code> <code>dict</code> <p>The message sent by the client.</p> required"},{"location":"api-reference/backend/#backend.molab_backend.main.main","title":"<code>main()</code>","text":"<p>Main function to run the FastAPI app using Uvicorn.</p>"},{"location":"api-reference/backend/#backend.molab_backend.main.register_client","title":"<code>register_client(websocket)</code>  <code>async</code>","text":"<p>WebSocket endpoint to register clients.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The WebSocket connection for the client.</p> required"},{"location":"api-reference/backend/#backend.molab_backend.main.register_worker","title":"<code>register_worker(websocket)</code>  <code>async</code>","text":"<p>WebSocket endpoint to register workers.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>WebSocket</code> <p>The WebSocket connection for the worker.</p> required"},{"location":"api-reference/inference-worker/","title":"Inference Worker","text":"<p>The <code>MotionInferenceWorker</code> is the core of MoLab's inference system. It is responsible for generating motion sequences based on the given inference parameters as defined in the <code>InferenceArgs</code>. The output is defined by the <code>InferenceResults</code> and contains both the input and output motion sequences.</p> <p>As underlying motion generation model we use a fork of <code>setarehc/diffusion-motion-inbetweening</code> aka CondMDI.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker","title":"<code>models.condmdi.molab_condmdi.inference_worker</code>","text":""},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.InferenceArgs","title":"<code>InferenceArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GenerateOptions</code>, <code>CondSyntOptions</code>, <code>CustomSyntOptions</code></p> <p>Arguments and options for motion inference.</p> <p>Attributes:</p> Name Type Description <code>text_prompt</code> <code>str</code> <p>Input text for motion generation.</p> <code>num_samples</code> <code>int</code> <p>The number of motion samples to generate. Default is 3.</p> <code>packed_motion</code> <code>dict[int, list]</code> <p>A mapping of frame indices to poses (J+1, 3), where each pose is defined by the root position and all J joint rotations. When no motion is supplied it will be initialized as \"empty\" motion. Alternatively you can use <code>bvh_path</code> to load a motion from file.</p> <code>unpack_mode</code> <code>str</code> <p>Mode for unpacking and converting <code>packed_motion</code> to features. Choices are \"step\" (stepped animation) and \"linear\" (interpolation). Default is \"linear\".</p> <code>unpack_randomness</code> <code>float</code> <p>Randomness applied during unpacking. Default is 0.0.</p> <code>editable_features</code> <code>str</code> <p>Indicates which features of the input are observed. Default is \"pos_rot_vel\", alternatives are \"pos_rot\" or just \"pos\".</p> <code>bvh_path</code> <code>str</code> <p>Path to the BVH file. Needs to be used in combination with <code>edit_mode</code> to specify the masking. Can not be used together with <code>packed_motion</code>.</p> <code>edit_mode</code> <code>str</code> <p>Masking strategy for the BVH input motion. For all options see <code>parser_util.CondSyntOptions</code>.</p> <code>jacobian_ik</code> <code>bool</code> <p>Flag to switch from Basic to Jacobian IK. Default is False.</p> <code>foot_ik</code> <code>bool</code> <p>Flag to enable Foot IK to reduce foot sliding. Default is False.</p> <code>imputate</code> <code>bool</code> <p>Flag to enable imputation between inference steps. Default is False.</p> <code>reconstruction_guidance</code> <code>bool</code> <p>Flag to enable reconstruction guidance during inference. Default is False.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.InferenceResults","title":"<code>InferenceResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Inference results containing samples and input motions.</p> <p>Attributes:</p> Name Type Description <code>root_positions</code> <code>list</code> <p>List of root positions.</p> <code>joint_rotations</code> <code>list</code> <p>List of joint rotations.</p> <code>obs_root_positions</code> <code>list</code> <p>List of observed root positions.</p> <code>obs_joint_rotations</code> <code>list</code> <p>List of observed joint rotations.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.ModelArgs","title":"<code>ModelArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseOptions</code>, <code>DataOptions</code>, <code>ModelOptions</code>, <code>DiffusionOptions</code>, <code>TrainingOptions</code>, <code>SamplingOptions</code></p> <p>Contains Mostly Model Options:</p> <ul> <li><code>BaseOptions</code> (<code>cuda</code>, <code>device</code>, <code>seed</code>)</li> <li><code>DataOptions</code> (Dataset Type and Path, Data Representation, Augmentation, ...)</li> <li><code>ModelOptions</code> (Model Architecture)</li> <li><code>DiffusionOptions</code> (Diffusion Hyperparameters)</li> <li><code>TrainingOptions</code> (Save path, Batchsize, LR, Loss Weights, ...)</li> <li><code>SamplingOptions</code> (Model and Output Path, Samples and Reps, CFG Guidance)</li> </ul>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.MotionInferenceWorker","title":"<code>MotionInferenceWorker</code>","text":""},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.MotionInferenceWorker.__init__","title":"<code>__init__(name, model_args)</code>","text":"<p>Initialize the worker and parse arguments without starting the model.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.MotionInferenceWorker.get_output_path","title":"<code>get_output_path(infer_config)</code>","text":"<p>Get the output path for the results of the inference.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.MotionInferenceWorker.infer","title":"<code>infer(infer_config, save_results=True)</code>","text":"<p>Infer using InferenceArgs and return InferenceResults, optionally save intermediate results to output directory.</p> <p>Parameters:</p> Name Type Description Default <code>infer_config</code> <code>InferenceArgs</code> <p>The arguments for motion inference.</p> required <code>save_results</code> <code>bool</code> <p>Whether to save outputs to disk. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InferenceResults</code> <code>InferenceResults</code> <p>The generated motion.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.MotionInferenceWorker.start","title":"<code>start()</code>","text":"<p>Start the model and load the checkpoint.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.get_abs_data_from_jointpos","title":"<code>get_abs_data_from_jointpos(joint_positions)</code>","text":"<p>Convert joint positions to HML3D_abs format.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.get_jointpos_from_bvh","title":"<code>get_jointpos_from_bvh(filepath)</code>","text":"<p>Load a BVH file and convert it to HML3D_abs format.</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.unpack_motion","title":"<code>unpack_motion(packed_motion, mode='linear', randomness=0.0)</code>","text":"<p>Unpack the packed motion input and return the joint positions and mask.</p> Packed Motion Format <ul> <li>A dictionary mapping frame indices to packed poses</li> <li>A packed pose contains the root position followed by all 22 joint rotations</li> <li>Values stored as <code>nan</code> indicate sparse keyframes and are converted to a joint mask</li> </ul> <p>Parameters:</p> Name Type Description Default <code>packed_motion</code> <code>dict[int, list]</code> <p>Packed motion data</p> required <code>mode</code> <code>str</code> <p>Interpolation mode, either \"linear\" or \"step\". Defaults to \"linear\".</p> <code>'linear'</code> <code>randomness</code> <code>float</code> <p>Random noise to add to the missing values. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Root positions (n_frames, 3)</p> <code>ndarray</code> <p>np.ndarray: Joint rotations (n_frames, 22, 3)</p> <code>ndarray</code> <p>np.ndarray: Joint mask (n_frames, 22, 1)</p>"},{"location":"api-reference/inference-worker/#models.condmdi.molab_condmdi.inference_worker.unpacked_motion_to_jointpos","title":"<code>unpacked_motion_to_jointpos(root_pos, rotations)</code>","text":"<p>Convert the unpacked motion to joint positions using the BVH template and Forward Kinematics.</p> <p>This is based on the following list of assumptions:</p> <ul> <li>The skeleton is the same as in <code>template.bvh</code> (22 joints)<ul> <li>This is used to get <code>parents</code>, <code>offsets</code> and <code>names</code></li> </ul> </li> <li>Frametime is 1/20s (as per HML3D dataset)</li> <li>Rotation order is XYZ</li> </ul>"},{"location":"api-reference/websocket-worker/","title":"WebSocket Worker","text":"<p>The <code>WebSocketWorker</code> encapsulates and serves a single instance of the <code>MotionInferenceWorker</code>.</p> <p>For setup, it requires the <code>backend_host</code> and <code>backend_port</code>, as well as which <code>checkpoint</code> to load for the inference worker.</p> <p>We plan to add more checkpoints in the future, currently there are only two checkpoints available, both from the original CondMDI repository:</p> <ul> <li><code>random_frames</code> (Default)<ul> <li>Trained on in-betweening full poses on random frames.</li> <li>Good for full-body in-betweening.</li> </ul> </li> <li><code>random_joints</code><ul> <li>Trained on in-betweening random joints (partial poses) on random frames.</li> <li>Good for in-betweening partial poses.</li> </ul> </li> </ul>"},{"location":"api-reference/websocket-worker/#models.condmdi.molab_condmdi.websocket_worker","title":"<code>models.condmdi.molab_condmdi.websocket_worker</code>","text":""},{"location":"api-reference/websocket-worker/#models.condmdi.molab_condmdi.websocket_worker.WebSocketWorker","title":"<code>WebSocketWorker</code>","text":""},{"location":"api-reference/websocket-worker/#models.condmdi.molab_condmdi.websocket_worker.WebSocketWorker.__init__","title":"<code>__init__(backend_host='localhost', backend_port=8000, checkpoint='random_frames')</code>","text":"<p>Initialize the WebSocketWorker.</p> <p>Parameters:</p> Name Type Description Default <code>backend_host</code> <code>str</code> <p>The hostname of the backend server. Defaults to \"localhost\".</p> <code>'localhost'</code> <code>backend_port</code> <code>int</code> <p>The port number of the backend server. Defaults to 8000.</p> <code>8000</code> <code>checkpoint</code> <code>str</code> <p>The name of the checkpoint to be used, current choices are \"random_frames\" (Default) or \"random_joints\".</p> <code>'random_frames'</code> <p>Attributes:</p> Name Type Description <code>inference_worker</code> <code>None</code> <p>Placeholder for the inference worker.</p> <code>uri</code> <code>str</code> <p>The WebSocket URI for registering the worker.</p> <code>checkpoint</code> <code>str</code> <p>The name of the checkpoint.</p> <code>checkpoint_path</code> <code>Path</code> <p>The path to the model checkpoint file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the model checkpoint file is not found at the specified path.</p>"},{"location":"api-reference/websocket-worker/#models.condmdi.molab_condmdi.websocket_worker.WebSocketWorker.serve","title":"<code>serve()</code>  <code>async</code>","text":"<p>Connect to the gateway and wait for inference requests.</p>"},{"location":"api-reference/websocket-worker/#models.condmdi.molab_condmdi.websocket_worker.WebSocketWorker.setup","title":"<code>setup()</code>","text":"<p>Start the <code>MotionInferenceWorker</code>.</p>"},{"location":"getting-started/first-steps/","title":"First Steps","text":"<p>After installing MoLab, follow the steps below to get everything up and running.</p>"},{"location":"getting-started/first-steps/#starting-backend-and-worker","title":"Starting Backend and Worker","text":"<p>Hint</p> <p>This will start the backend and worker locally. For a distributed setup see the Deployment Guide.</p> <p>First start the backend server:</p> <pre><code>$ just run-backend\n\ud83d\ude80 Running the backend\n...\n</code></pre> <p>Then start a worker and wait for it to connect to the backend:</p> <pre><code>$ just run-worker\n\ud83d\ude80 Running the worker\n...\n</code></pre> <p>Now everything is set up and you can connect to the backend using any of the clients below.</p>"},{"location":"getting-started/first-steps/#starting-a-client","title":"Starting a Client","text":"<p>Currently MoLab supports these clients with different use cases:</p> <ul> <li>MoLab Sequencer: The interface built with Godot<ul> <li>Aimed at producers and animators</li> <li>Generate animations from text descriptions</li> <li>Layer-based editing and transitions</li> </ul> </li> <li>MoLab Maya Plugin: Integration for Autodesk Maya<ul> <li>Aimed at animators</li> <li>Infer from the Maya Script Editor</li> <li>Feed key poses and generate in-betweens</li> </ul> </li> <li>MoLab API: Directly connect to the backend<ul> <li>Aimed at developers</li> <li>Use the API for fine-grained control</li> <li>Integrate MoLab into your own applications</li> </ul> </li> </ul>"},{"location":"getting-started/first-steps/#molab-sequencer","title":"MoLab Sequencer","text":"<p>Either start the Godot project from the editor or export the project to a standalone application, as described in the installation guide. You will be greeted with an empty composition and in the log output you should see a successful connection to the backend.</p> <p></p> <p>You are now ready to start generating and layering animations. First add a new \"ML Source\" via the \"Add New Source\" Button. Then select the source and type a description into the \"Prompt Text\" field and hit Enter. Finally you can press the \"Process\" button.</p> <p>This process will take from 20s to 2min, depending on your available GPU resources. If you leave the text prompt empty, the inference time will be halved. Afterwards you can switch between the generated samples and play them back.</p> <p></p> <p>You can also supply input motion for the generation process by using the \"In Offset\" or \"Out Offset\" properties. The model will then try to fill the gaps according to the input motion and the prompt text.</p> <p></p> <p>Now you know the basics of the MoLab Sequencer and can start generating and layering animations. For more details and explanations, see the MoLab Sequencer documentation.</p>"},{"location":"getting-started/first-steps/#molab-maya-plugin","title":"MoLab Maya Plugin","text":"<p>First open the <code>dcc/template.ma</code> scene in Maya to start with a compatible skeleton and an animation with two keyframes as example poses.</p> <p></p> <p>Then load the MoLab Maya Plugin by pasting the contents of <code>dcc/maya_shelf_script.py</code> into the Maya Script Editor and running it. This will open up a new window with the MoLab Maya Plugin.</p> <p></p> <p>First supply a Backend URI, the default is fine since we are running everything locally in this example. Then press the \"Connect\" Button to establish a connection to the backend. The button should change to \"Connected\" if the connection was successful. Now select the characters <code>Hips</code> joint in Maya and press the \"Pick\" Button in the plugin. This will validate the skeleton and set up the plugin for inference. Next, adjust the input range to your desired keyframes and optionally add a prompt text. In this example we leave the prompt text empty and do pure in-betweening.</p> <p>Finally, press the \"Generate\" Button on the bottom and wait for the inference to finish. Again, this can take from 20s to 2min, depending on your available GPU resources. If you leave the text prompt empty, the inference time will be halved. After a while you should see a duplicated character per sample and can now inspect the generated animations.</p> <p></p> <p>For further details on how to use the plugin, see the MoLab Maya Plugin documentation.</p>"},{"location":"getting-started/first-steps/#next-steps","title":"Next Steps","text":"<p>Now that you have everything set up, you can start using MoLab to generate animations. For more detailed information on how to use the clients, refer to their respective documentations: MoLab Sequencer, MoLab Maya Plugin or MoLab API.</p>"},{"location":"getting-started/installation/","title":"Installing MoLab","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> macOS and LinuxWindows <ul> <li>uv as Python Package Manager</li> <li>Godot Engine 4 for the Frontend</li> <li>just as Command Runner<ul> <li>can be installed via <code>uv tool install rust-just</code></li> </ul> </li> </ul> <ul> <li>uv as Python Package Manager</li> <li>Godot Engine 4 for the Frontend</li> <li>just as Command Runner<ul> <li>can be installed via <code>uv tool install rust-just</code></li> </ul> </li> <li>bash (or any other shell) for the scripts downloading the pretrained models</li> </ul>"},{"location":"getting-started/installation/#installing-backend-and-worker","title":"Installing Backend and Worker","text":"<p>First clone the MoLab repository:</p> <pre><code>git clone https://github.com/JasonNero/MoLab.git\n</code></pre> <p>Then run the <code>install</code> task to setup the python environments followed by the <code>download</code> task to download the pretrained model checkpoints:</p> <pre><code>cd MoLab\njust install\njust download\n</code></pre>"},{"location":"getting-started/installation/#installing-a-client","title":"Installing a Client","text":"<p>With the backend and worker set up, you can now install a client to connect to the backend. You have two options here: the frontend built with Godot or the Maya plugin.</p>"},{"location":"getting-started/installation/#building-the-sequencer","title":"Building the Sequencer","text":"<p>MoLab Sequencer is using Mixamo characters for visualization. Due to license restrictions, this repository does not distribute the required 3D character model.</p> <p>First, please download the \"Akai e espiritu\" model from the Mixamo Website and save it under <code>frontend/res/models/akai_e_espiritu.fbx</code>.</p> <p>Then you can open the <code>frontend</code> project in Godot and re-import the model by double-clicking the <code>akai_e_espiritu.fbx</code> file in the \"File System\" tab and hitting \"Reimport\".</p> <p>Finally, you can either run from the editor or export the project to a standalone application.</p> <p>Hint</p> <p>You might need to reset any changes to the <code>akai_e_espiritu.fbx.import</code> file since Godot might have changed the import settings if you started Godot before downloading the model.</p>"},{"location":"getting-started/installation/#installing-the-maya-plugin","title":"Installing the Maya Plugin","text":"macOSWindows <p>First install the <code>qtpy</code> package via <code>pip</code> using Maya's Python interpreter, make sure to adapt the path to your Maya version:</p> <pre><code>/Applications/Autodesk/maya2024/Maya.app/Contents/bin/mayapy -m pip install qtpy\n</code></pre> <p>Then copy the <code>dcc/molab_maya</code> folder to the Maya scripts directory, again adapt the path to your Maya version:</p> <pre><code>cp -r dcc/molab_maya ~/Library/Preferences/Autodesk/maya/2024/scripts\n</code></pre> <p>First install the <code>qtpy</code> package via <code>pip</code> using Maya's Python interpreter, make sure to adapt the path to your Maya version:</p> <pre><code>\"C:\\Program Files\\Autodesk\\Maya2024\\bin\\mayapy.exe\" -m pip install qtpy\n</code></pre> <p>Then copy the <code>dcc/molab_maya</code> folder to the Maya scripts directory, again adapt the path to your Maya version:</p> <pre><code>cp -r dcc/molab_maya \"C:\\Users\\%USERNAME%\\Documents\\maya\\scripts\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>See the First Steps guide to learn how to spin up the backend and worker, and how connect to the clients.</p>"},{"location":"usage/inference-parameters/","title":"Inference Parameters","text":""},{"location":"usage/inference-parameters/#overview","title":"Overview","text":"<p>The most common inference arguments are:</p> <ul> <li><code>text_prompt</code>: The description of the desired motion.</li> <li><code>num_samples</code>: The number of samples to generate.</li> <li><code>packed_motion</code> The input motion to be used as a starting point for the inference.</li> </ul> <p>Instead of <code>packed_motion</code>, you can also infer from a local BVH file using:</p> <ul> <li><code>bvh_file</code>: The path to the local BVH file to be used as input motion.</li> <li><code>edit_mode</code> The masking mode applied onto the input motion.</li> </ul> <p>Further options that might be interesting to adjust are:</p> <ul> <li><code>unpack_mode</code>: How to unpack the motion and fill the missing values, can be stepped or linearly interpolated.</li> <li><code>unpack_randomness</code>: The randomness applied during unpacking.</li> <li><code>editable_features</code>: The features (joint positions, rotations and velocities) that are extracted from the input motion.</li> </ul> <p>Finally, there are some experimental options:</p> <ul> <li><code>jacobian_ik</code>: Whether to use Jacobian IK instead of Basic IK.</li> <li><code>foot_ik</code>: Whether to use foot IK.</li> </ul> <p>For an extended list of available parameters see the API Reference, specifically the InferenceArgs class.</p>"},{"location":"usage/maya-plugin/","title":"Using the MoLab Plugin for Maya","text":"<p>The Maya Plugin serves a similar functionality like the MoLab Sequencer, but is focused more on in-betweening than on generating animations from scratch. It is aimed at animators who want to speed up their workflow by generating in-betweens for key poses.</p>"},{"location":"usage/maya-plugin/#setup","title":"Setup","text":"<p>To use the MoLab Maya Plugin, you need to install it first (see the installation guide). Then you can start up the UI using the snippet in <code>dcc/maya_shelf_script.py</code> in the Maya Script Editor. Feel free to add this script to your shelf for easy access.</p>"},{"location":"usage/maya-plugin/#usage","title":"Usage","text":"<p>The basic workflow is as follows:</p> <ul> <li>Provide a backend URI</li> <li>Select the Hip bone of the skeleton and hit the \"Pick\" button</li> <li>Decide whether to use input motion and if so, provide start and end frames</li> <li>Decide whether to use a text prompt or not</li> <li>Finally supply the amount of samples and hit the \"Generate\" button</li> </ul> <p>There are also advanced options available, which are explained further in the Inference Parameter Guide.</p> <p></p>"},{"location":"usage/sequencer/","title":"Using the MoLab Sequencer","text":""},{"location":"usage/sequencer/#composition-and-sources","title":"Composition and Sources","text":"<p>A Composition in MoLab Sequencer is a collection of Motion Sources, either loaded from file or generated by the CondMDI model. Using the well known shortcuts Ctrl+O and Ctrl+S, you can open and save compositions respectively. To start with a new, empty composition, use Ctrl+N.</p> <p>Every source has a in-point and out-point that define the total length and position in the timeline. Furthermore each source has a in-offset and out-offset that define the start and end of the motion within the source.</p> <p>When layering sources, only the inner range of a source is applied, while the outer range is ignored.</p> <p>The \"Affects Post-Range\" option defines whether underlying sources are affected of root motion changes in the case of a source that ends before the underlying source. Disabling this option will result in a snap-back to the original position of the underlying source, while enabling it will result in a smooth transition.</p> <p>Currently, there are two types of sources available:</p> <ul> <li>File Source: A motion source loaded from a file.</li> <li>ML Source: A motion source generated by the CondMDI model.</li> </ul>"},{"location":"usage/sequencer/#file-source","title":"File Source","text":"<p>When using the File Source, the user can select a file from the file dialog and the source will be loaded into the timeline.</p> <p>Note</p> <p>The BVH importer has been disabled in the current version due to rig incompatibilities. We are working on a solution to enable this feature in the future. Please use glTF files for now.</p>"},{"location":"usage/sequencer/#ml-source","title":"ML Source","text":"<p>When adding a ML Source, a source of 197 frames is added to the timeline (this is the maximum sequence length for CondMDI). This source can optionally be provided with a description of the desired motion in the \"Prompt Text\" field.</p> <p>In contrast to the usual source, the in-offset and out-offset properties allow you to specify the ranges that should be used as input/context for the generation process and which range should be generated:</p> <ul> <li>The inner range is the range that will be generated by the model</li> <li>The outer ranges are the context for the generation process.</li> </ul> <p>Finally, upon hitting the \"Process\" button in the properties panel, the source is sent to the backend for generation.</p> <p>Note</p> <p>Currently there is no progress feedback in the UI for the generation process, so please be patient and check the worker logs for progress.  Once the result is ready, the source will be deselected as an indicator.</p> <p>After the generation process is finished, the user can select one of the generated motions in the \"Selected Sample\" Dropdown and play it back in the timeline.</p>"},{"location":"usage/sequencer/#using-the-timeline","title":"Using the Timeline","text":"<p>The top of the timeline contains playback controls and the current playback time or frame. Below that is the timeline itself, where sources can be added and arranged. You can drag sources around in the timeline to change their position or move their in/out points as well as in/out offsets.</p> <p>Using the scroll wheel, you can move the timeline horizontally to navigate. To zoom in and out, hold the Shift key and use the scroll wheel.</p> <p>You can also use Alt+Space to play and pause the timeline as well as Alt+Left to jump to the previous frame or the next frame using Alt+Right.</p> <p>A different way to change the current time is to use the time input field at the top of the timeline or drag the playback head to the desired position. To grab the playback head, click and drag the line below the last source in the timeline. (This is a known issue and will be fixed in a future release.)</p>"},{"location":"usage/sequencer/#export-to-gltf-20","title":"Export to glTF 2.0","text":"<p>After you are satisfied with your composition, you can export it to a glTF file by using the Ctrl+E shortcut. This will open a file dialog where you can select the location and name of the file to save. The resulting glTF file will contain the character and the composition as a single animation clip.</p>"}]}